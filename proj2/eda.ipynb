{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = './Dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution for train\n",
      "bed: 1713\n",
      "bird: 1731\n",
      "cat: 1733\n",
      "dog: 1746\n",
      "down: 2359\n",
      "eight: 2352\n",
      "five: 2357\n",
      "four: 2372\n",
      "go: 2372\n",
      "happy: 1742\n",
      "house: 1750\n",
      "left: 2353\n",
      "marvin: 1746\n",
      "nine: 2364\n",
      "no: 2375\n",
      "off: 2357\n",
      "on: 2367\n",
      "one: 2370\n",
      "right: 2367\n",
      "seven: 2377\n",
      "sheila: 1734\n",
      "six: 2369\n",
      "stop: 2380\n",
      "three: 2356\n",
      "tree: 1733\n",
      "two: 2373\n",
      "up: 2375\n",
      "wow: 1745\n",
      "yes: 2377\n",
      "zero: 2376\n",
      "_background_noise_: 7\n",
      "Class distribution for test\n",
      "audio: 158538\n"
     ]
    }
   ],
   "source": [
    "classCounts = {\"train\":{},\"test\":{}}\n",
    "dirPath = os.path.join(DATASET_DIR,'test')\n",
    "for classFolder in os.listdir(dirPath):\n",
    "    classCounts['test'][classFolder] = len(os.listdir(os.path.join(dirPath, classFolder)))\n",
    "\n",
    "dirPath = os.path.join(DATASET_DIR,'train/audio')\n",
    "for classFolder in os.listdir(dirPath):\n",
    "    classCounts['train'][classFolder] = len(os.listdir(os.path.join(dirPath, classFolder)))\n",
    "\n",
    "\n",
    "for splitName, splitObject in classCounts.items():\n",
    "    print(f\"Class distribution for {splitName}\")\n",
    "    for subfolder, count in splitObject.items():\n",
    "        print(f\"{subfolder}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating train, valid, test split from the train folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_WAVS_PER_CLASS = 2**27 - 1  # ~134M\n",
    "\n",
    "def which_set(filename, validation_percentage, testing_percentage):\n",
    "  \"\"\"Determines which data partition the file should belong to.\n",
    "\n",
    "  We want to keep files in the same training, validation, or testing sets even\n",
    "  if new ones are added over time. This makes it less likely that testing\n",
    "  samples will accidentally be reused in training when long runs are restarted\n",
    "  for example. To keep this stability, a hash of the filename is taken and used\n",
    "  to determine which set it should belong to. This determination only depends on\n",
    "  the name and the set proportions, so it won't change as other files are added.\n",
    "\n",
    "  It's also useful to associate particular files as related (for example words\n",
    "  spoken by the same person), so anything after '_nohash_' in a filename is\n",
    "  ignored for set determination. This ensures that 'bobby_nohash_0.wav' and\n",
    "  'bobby_nohash_1.wav' are always in the same set, for example.\n",
    "\n",
    "  Args:\n",
    "    filename: File path of the data sample.\n",
    "    validation_percentage: How much of the data set to use for validation.\n",
    "    testing_percentage: How much of the data set to use for testing.\n",
    "\n",
    "  Returns:\n",
    "    String, one of 'training', 'validation', or 'testing'.\n",
    "  \"\"\"\n",
    "  base_name = os.path.basename(filename)\n",
    "  # We want to ignore anything after '_nohash_' in the file name when\n",
    "  # deciding which set to put a wav in, so the data set creator has a way of\n",
    "  # grouping wavs that are close variations of each other.\n",
    "  hash_name = re.sub(r'_nohash_.*$', '', base_name).encode()\n",
    "  # This looks a bit magical, but we need to decide whether this file should\n",
    "  # go into the training, testing, or validation sets, and we want to keep\n",
    "  # existing files in the same set even if more files are subsequently\n",
    "  # added.\n",
    "  # To do that, we need a stable way of deciding based on just the file name\n",
    "  # itself, so we do a hash of that and then use that to generate a\n",
    "  # probability value that we use to assign it.\n",
    "  hash_name_hashed = hashlib.sha1(hash_name).hexdigest()\n",
    "  percentage_hash = ((int(hash_name_hashed, 16) %\n",
    "                      (MAX_NUM_WAVS_PER_CLASS + 1)) *\n",
    "                     (100.0 / MAX_NUM_WAVS_PER_CLASS))\n",
    "  if percentage_hash < validation_percentage:\n",
    "    result = 'validation'\n",
    "  elif percentage_hash < (testing_percentage + validation_percentage):\n",
    "    result = 'testing'\n",
    "  else:\n",
    "    result = 'training'\n",
    "  return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFileClass(filePath):\n",
    "    dir = os.path.dirname(filePath)\n",
    "    return dir.split('\\\\')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_per= 10\n",
    "test_per =10\n",
    "valid_fileNames=[]\n",
    "test_fileNames=[]\n",
    "train_fileNames=[]\n",
    "\n",
    "for subdir, dirs, files in os.walk('./Dataset/train/audio'):\n",
    "    for file in files:\n",
    "        # directory = subdir.split('\\\\')[-1]\n",
    "        match(which_set(file,valid_per,test_per)):\n",
    "            case 'training':\n",
    "                train_fileNames.append(os.path.join(subdir,file))\n",
    "            case 'validation':\n",
    "                valid_fileNames.append(os.path.join(subdir,file))\n",
    "            case 'testing':\n",
    "                test_fileNames.append(os.path.join(subdir,file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'getFileClass' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgetFileClass\u001b[49m(valid_fileNames[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'getFileClass' is not defined"
     ]
    }
   ],
   "source": [
    "getFileClass(valid_fileNames[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can handle the files the way we want, by either loading them all to memory or creating a stream thingy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
