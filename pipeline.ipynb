{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo:\n",
    "\n",
    "3. Prepare the whole pipeline\n",
    "    1. Data augmentation\n",
    "        - First take the unaugmented original dataset and proceed\n",
    "        -  Augment the data with a predefined seed for each of the following techniques: rotation, flipping, contrast, brightness change, random erasing\n",
    "    2. Choose hyperparameters\n",
    "        - For the hyper-parameters related to the training process we chose batch size, learning rate, and number of epochs\n",
    "        - For the hyper-parameters related to Regularization we decided to use L2 Regularization (Weight Decay) and Dropout Rate\n",
    "    3. Train each of the prepared models on the augmented dataset for a chosen augmentation technique\n",
    "    4. Test and collect data regarding modelsâ€™ performance on the augmented dataset\n",
    "    5. Repeat several times (>=3) from 3.\n",
    "    6. Choose different values for hyperparameters and start from 3.\n",
    "    7. Choose the next augmentation technique and start from 2.\n",
    "    8. Repeat the process starting from 1. several times (>=3) with a different seed each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'distutils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, GlobalAveragePooling2D, BatchNormalization\n",
      "File \u001b[1;32mc:\\Users\\prusak.patryk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\__init__.py:30\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03mTop-level module of TensorFlow. By convention, we refer to this module as\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m`tf` instead of `tensorflow`, following the common practice of importing\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;124;03mthis file with a file generated from [`api_template.__init__.py`](https://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/api_template.__init__.py)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-bad-import-order,protected-access,g-import-not-at-top\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdistutils\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_distutils\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_inspect\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'distutils'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from enum import Enum\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, BatchNormalization\n",
    "from efficientnet.tfkeras import EfficientNetB0\n",
    "from tensorflow.keras.applications import MobileNetV3Large\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move 5400 images from each class from valid to train\n",
    " - There is a safety check, if it has been already done it won't do it again :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = './Dataset/valid'\n",
    "\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    if subdir != rootdir:\n",
    "        for subsubdir, subdirs, files in os.walk(subdir):\n",
    "            if len(files) < 5400:\n",
    "                break;\n",
    "            for i in range(5400):\n",
    "                os.rename(os.path.join(os.path.join(\"./Dataset/valid\",os.path.basename(subsubdir)),files[i]), os.path.join(os.path.join(\"./Dataset/train\",os.path.basename(subsubdir)),files[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentationTechnique(Enum):\n",
    "    NoAugmentation = 0\n",
    "    Rotation = 1\n",
    "    Flipping = 2\n",
    "    Contrast = 3\n",
    "    Brightness = 4\n",
    "    RandomErasing = 5\n",
    "\n",
    "class ModelType(Enum):\n",
    "    MobileNet =1\n",
    "    EfficientNet = 2\n",
    "\n",
    "class Model:\n",
    "    def fit(self,batch_size,epochs,train_data_generator,valid_data_generator):\n",
    "        pass\n",
    "    def predict(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "# TODO: implement those classes for models\n",
    "def CustomMobileNetModel(Model):\n",
    "    def __init__(self,optimizer,loss,metrics):\n",
    "        # Load MobileNetV3Large without top classification layer\n",
    "        base_model = MobileNetV3Large(include_top=False, weights='imagenet', input_shape=(32, 32, 3))\n",
    "\n",
    "        # Freeze the base model layers\n",
    "        base_model.trainable = False\n",
    "\n",
    "        # Add additional layers on top of MobileNetV3Large\n",
    "        model = tf.keras.Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(1024, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dense(512, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dense(10, activation='softmax')  # Output layer with size 10 for classification\n",
    "        ])\n",
    "        # Compile the model\n",
    "        model.compile(optimizer, loss, metrics)\n",
    "        self.model=model\n",
    "\n",
    "    def fit(self,batch_size,epochs,train_data_generator,valid_data_generator):\n",
    "        self.model.fit(\n",
    "        train_data_generator,\n",
    "        steps_per_epoch=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=valid_data_generator,\n",
    "        validation_steps=batch_size\n",
    "        )\n",
    "    def predict(self,test_data_generator):\n",
    "        self.model.predict(test_data_generator, steps=len(test_data_generator))\n",
    "\n",
    "def CustomEfficientNetModel(Model):\n",
    "    def __init__(self):\n",
    "        # Load MobileNetV3Large without the top classification layer\n",
    "        base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
    "        \n",
    "        # Freeze the base model layers\n",
    "        base_model.trainable = False\n",
    "        # Add additional layers on top of EfficientNet\n",
    "        model = tf.keras.Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(1024, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dense(512, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dense(10, activation='softmax')  # Output layer with size 10 for classification\n",
    "        ])\n",
    "        # Compile the model\n",
    "        optimizer = Adam(learning_rate=0.001)\n",
    "        model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        self.model=model\n",
    "    def fit(self,batch_size,epochs,train_data_generator,valid_data_generator):\n",
    "        self.model.fit(\n",
    "        train_data_generator,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=valid_data_generator,\n",
    "        )\n",
    "    def predict(self,test_data_generator):\n",
    "        predictions = self.model.predict(test_data_generator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def createModel(modelType, optimizer, loss, metrics)->Model:\n",
    "    match modelType:\n",
    "        # TODO: Implement the creation of these models and then return the model\n",
    "        case ModelType.MobileNet:\n",
    "            return CustomMobileNetModel(optimizer,loss,metrics)\n",
    "        case ModelType.EfficientNet:\n",
    "            return NotImplementedError\n",
    "\n",
    "def augumentData(data, technique, seed):\n",
    "    match technique:\n",
    "        case AugmentationTechnique.Rotation:\n",
    "            print(\"Hello\")\n",
    "\n",
    "    return data\n",
    "\n",
    "def getAccuracy(y_result, y_test):\n",
    "    correct_amount =0 \n",
    "    for i, result in enumerate(y_result):\n",
    "        if result == y_test[i]:\n",
    "            correct_amount+=1\n",
    "    return correct_amount/len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrays containing different hyper-parameter values\n",
    "\n",
    "TODO: Handle hyperparameters related to regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training process\n",
    "batchSizes =[]\n",
    "learningRates = []\n",
    "numberOfEpochs =[]\n",
    "\n",
    "# regularization\n",
    "\n",
    "\n",
    "\n",
    "# augmentation\n",
    "augmentationTechniques =[AugmentationTechnique.NoAugmentation,AugmentationTechnique.Rotation,AugmentationTechnique.Flipping,AugmentationTechnique.Contrast,AugmentationTechnique.Brightness,AugmentationTechnique.RandomErasing]\n",
    "\n",
    "\n",
    "REPETITIONS = 3\n",
    "seeds = [123,42,56]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main pipeline loop\n",
    "\n",
    "TODO: repeat the experiments 3 times, different seed each time. I want to first test if it works once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performExperiment(modelType,X,y, X_test, y_test):\n",
    "    results = []\n",
    "    for batchSize in batchSizes:\n",
    "        for learningRate in learningRates:\n",
    "            for epochNumber in numberOfEpochs:\n",
    "                for augmentation in augmentationTechniques:\n",
    "                    augX = augumentData(X,augmentation,seeds[0])\n",
    "                    # create the model\n",
    "                    model = createModel(modelType,optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "                    # train the model \n",
    "                    model.fit(augX, y,batch_size=batchSize,epochs=epochNumber)\n",
    "\n",
    "                    # get accuracy\n",
    "                    y_pred = model.predict(X_test)\n",
    "                    accuracy = getAccuracy(y_pred,y_test)\n",
    "\n",
    "                    # append to results\n",
    "                    results.append({'accuracy':accuracy,'augmentation': augmentation,'batchSize':batchSize,'learningRate':learningRate,'numberOfEpochs':numberOfEpochs})\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNUSED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class HyperParameter(Enum):\n",
    "    BatchSize=1\n",
    "    LearningRate =2\n",
    "    NumberOfEpochs =3\n",
    "\n",
    "class HyperParameters:\n",
    "    def __init__(self, batchSizes, learningRates, numberOfEpochs):\n",
    "        self.batchSizes=batchSizes\n",
    "        self.learningRates=learningRates\n",
    "        self.numberOfEpochs=numberOfEpochs\n",
    "        self.currentIndex =0\n",
    "    \n",
    "    def getNextHyperParameter(self):\n",
    "        if self.currentIndex<len(self.batchSizes):\n",
    "            return self.batchSizes[self.currentIndex], HyperParameter.BatchSize\n",
    "        elif self.currentIndex<(len(self.batchSizes+self.learningRates)):\n",
    "            return self.learningRates[self.currentIndex-len(self.batchSizes)], HyperParameter.LearningRate\n",
    "        elif self.currentIndex<(len(self.batchSizes)+len(self.learningRates)+len(self.numberOfEpochs)):\n",
    "            return self.num\n",
    "        \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
