{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Prepare the whole pipeline\n",
    "    1. Data augmentation\n",
    "        - First take the unaugmented original dataset and proceed\n",
    "        -  Augment the data with a predefined seed for each of the following techniques: rotation, flipping, contrast, brightness change, random erasing\n",
    "    2. Choose hyperparameters\n",
    "        - For the hyper-parameters related to the training process we chose batch size, learning rate, and number of epochs\n",
    "        - For the hyper-parameters related to Regularization we decided to use L2 Regularization (Weight Decay) and Dropout Rate\n",
    "    3. Train each of the prepared models on the augmented dataset for a chosen augmentation technique\n",
    "    4. Test and collect data regarding models’ performance on the augmented dataset\n",
    "    5. Repeat several times (>=3) from 3.\n",
    "    6. Choose different values for hyperparameters and start from 3.\n",
    "    7. Choose the next augmentation technique and start from 2.\n",
    "    8. Repeat the process starting from 1. several times (>=3) with a different seed each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from enum import Enum\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam, SGD, Lion\n",
    "from keras.regularizers import L2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, BatchNormalization, Dropout\n",
    "from efficientnet.tfkeras import EfficientNetB0\n",
    "from tensorflow.keras.applications import MobileNetV3Large\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move 5400 images from each class from valid to train\n",
    " - There is a safety check, if it has been already done it won't do it again :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = './Dataset/valid'\n",
    "\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    if subdir != rootdir:\n",
    "        for subsubdir, subdirs, files in os.walk(subdir):\n",
    "            if len(files) < 5400:\n",
    "                break;\n",
    "            for i in range(5400):\n",
    "                os.rename(os.path.join(os.path.join(\"./Dataset/valid\",os.path.basename(subsubdir)),files[i]), os.path.join(os.path.join(\"./Dataset/train\",os.path.basename(subsubdir)),files[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentationTechnique(Enum):\n",
    "    NoAugmentation = 0\n",
    "    Rotation = 1\n",
    "    Flipping = 2\n",
    "    Contrast = 3\n",
    "    Brightness = 4\n",
    "    RandomErasing = 5\n",
    "\n",
    "class ModelType(Enum):\n",
    "    MobileNet =1\n",
    "    EfficientNet = 2\n",
    "\n",
    "class OptimizerType(Enum):\n",
    "    Adam = 1\n",
    "    Sgd=2\n",
    "    Lion=3\n",
    "class RegularizationType(Enum):\n",
    "    NoRegularization = 1\n",
    "    L2 =2\n",
    "    WeightDecay=3\n",
    "def getRegularizer(regularizerType,value):\n",
    "    match regularizerType:\n",
    "        case RegularizationType.L2:\n",
    "            return L2(value)\n",
    "def getDenseLayer(model,regularization, activation,nodes):\n",
    "        match regularization['type']:\n",
    "            case RegularizationType.WeightDecay:\n",
    "                model.add(Dropout(rate=regularization['value']))\n",
    "                model.add(Dense(nodes, activation=activation))\n",
    "            case RegularizationType.NoRegularization:\n",
    "                model.add(Dense(nodes, activation=activation))\n",
    "            case RegularizationType.L2:\n",
    "                model.add(Dense(nodes, activation=activation, kernel_regularizer=getRegularizer(type=regularization['type'],value=regularization['value'])))\n",
    "\n",
    "class Model:\n",
    "    def fit(self,batch_size,epochs,train_data_generator,valid_data_generator):\n",
    "        pass\n",
    "    def predict(self,test_data_generator):\n",
    "        pass\n",
    "    def __init__(self,optimizer,loss,metrics):\n",
    "        pass\n",
    "\n",
    "# TODO: implement 3rd model\n",
    "    # TODO: How should we handle Droput and L2, should it be applied to each Dense layer?\n",
    "class CustomMobileNetModel(Model):\n",
    "    def __init__(self,optimizer,loss,metrics,regularizer):\n",
    "        # Load MobileNetV3Large without the top classification layer\n",
    "        base_model = MobileNetV3Large(include_top=False, weights='imagenet', input_shape=(32, 32, 3))\n",
    "\n",
    "        # Freeze the base model layers\n",
    "        base_model.trainable = False\n",
    "\n",
    "        # Add additional layers on top of MobileNetV3Large\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(base_model)\n",
    "        model.add(GlobalAveragePooling2D())\n",
    "\n",
    "        getDenseLayer(model=model,regularization=regularizer,activation='relu',nodes=1024)\n",
    "        model.add(BatchNormalization())\n",
    "        getDenseLayer(model=model,regularization=regularizer,activation='relu',nodes=512)\n",
    "        model.add(BatchNormalization())\n",
    "        getDenseLayer(model=model,regularization=regularizer,activation='softmax',nodes=10)\n",
    "            # Output layer with size 10 for classification\n",
    "        # Compile the model\n",
    "        model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "        self.model=model\n",
    "\n",
    "    def fit(self,batch_size,epochs,train_data_generator,valid_data_generator):\n",
    "        self.model.fit(\n",
    "        train_data_generator,\n",
    "        steps_per_epoch=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=valid_data_generator,\n",
    "        validation_steps=batch_size\n",
    "        )\n",
    "    def predict(self,test_data_generator):\n",
    "        return self.model.predict(test_data_generator ,steps=len(test_data_generator))\n",
    "    \n",
    "class CustomEfficientNetModel(Model):\n",
    "    def __init__(self,optimizer,loss,metrics, regularizer):\n",
    "        # Load MobileNetV3Large without the top classification layer\n",
    "        base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
    "        \n",
    "        # Freeze the base model layers\n",
    "        base_model.trainable = False\n",
    "        # Add additional layers on top of EfficientNet\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(base_model)\n",
    "        model.add(GlobalAveragePooling2D())\n",
    "        getDenseLayer(model=model,regularization=regularizer,activation='relu',nodes=1024)\n",
    "        model.add(BatchNormalization())\n",
    "        getDenseLayer(model=model,regularization=regularizer,activation='relu',nodes=512)\n",
    "        model.add(BatchNormalization())\n",
    "        getDenseLayer(model=model,regularization=regularizer,activation='softmax',nodes=10)\n",
    "            # Output layer with size 10 for classification\n",
    "        # Compile the model\n",
    "        model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "        self.model=model\n",
    "    def fit(self,batch_size,epochs,train_data_generator,valid_data_generator):\n",
    "        self.model.fit(\n",
    "        train_data_generator,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=valid_data_generator,\n",
    "        )\n",
    "    def predict(self,test_data_generator):\n",
    "        return self.model.predict(test_data_generator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/yu4u/cutout-random-erasing\n",
    "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
    "    def eraser(input_img):\n",
    "        if input_img.ndim == 3:\n",
    "            img_h, img_w, img_c = input_img.shape\n",
    "        elif input_img.ndim == 2:\n",
    "            img_h, img_w = input_img.shape\n",
    "\n",
    "        p_1 = np.random.rand()\n",
    "\n",
    "        if p_1 > p:\n",
    "            return input_img\n",
    "\n",
    "        while True:\n",
    "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
    "            r = np.random.uniform(r_1, r_2)\n",
    "            w = int(np.sqrt(s / r))\n",
    "            h = int(np.sqrt(s * r))\n",
    "            left = np.random.randint(0, img_w)\n",
    "            top = np.random.randint(0, img_h)\n",
    "\n",
    "            if left + w <= img_w and top + h <= img_h:\n",
    "                break\n",
    "\n",
    "        if pixel_level:\n",
    "            if input_img.ndim == 3:\n",
    "                c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
    "            if input_img.ndim == 2:\n",
    "                c = np.random.uniform(v_l, v_h, (h, w))\n",
    "        else:\n",
    "            c = np.random.uniform(v_l, v_h)\n",
    "\n",
    "        input_img[top:top + h, left:left + w] = c\n",
    "\n",
    "        return input_img\n",
    "\n",
    "    return eraser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def createModel(modelType, optimizer, loss, metrics, regularizer)->Model:\n",
    "    match modelType:\n",
    "        case ModelType.MobileNet:\n",
    "            return CustomMobileNetModel(optimizer=optimizer,loss=loss,metrics=metrics,regularizer=regularizer)\n",
    "        case ModelType.EfficientNet:\n",
    "            return NotImplementedError\n",
    "\n",
    "def set_seed(seed=0):\n",
    "    np.random.seed(seed) \n",
    "    tf.random.set_seed(seed) \n",
    "    random.seed(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = \"1\"\n",
    "    os.environ['TF_CUDNN_DETERMINISM'] = \"1\"\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "def getContrastChange():\n",
    "    def contrastChange(input_img):\n",
    "        contrast = np.randint(0,100)\n",
    "        f = 131*(contrast + 127)/(127*(131-contrast))\n",
    "        alpha_c = f\n",
    "        gamma_c = 127*(1-f)\n",
    "\n",
    "        input_img = cv2.addWeighted(input_img, alpha_c, input_img, 0, gamma_c)\n",
    "        return input_img\n",
    "    return contrastChange\n",
    "\n",
    "\n",
    "def augumentData(technique, seed):\n",
    "    match technique:\n",
    "        case AugmentationTechnique.NoAugmentation:\n",
    "            return ImageDataGenerator()\n",
    "        case AugmentationTechnique.Rotation:\n",
    "            return ImageDataGenerator(rotation_range=20)\n",
    "        case AugmentationTechnique.Flipping:\n",
    "            return ImageDataGenerator(horizontal_flip=True,vertical_flip=True)\n",
    "        case AugmentationTechnique.Brightness:\n",
    "            return ImageDataGenerator(brightness_range=[0.2,1.8])\n",
    "        case AugmentationTechnique.Contrast:\n",
    "            return ImageDataGenerator(preprocessing_function=getContrastChange())\n",
    "        case AugmentationTechnique.RandomErasing:\n",
    "            return ImageDataGenerator(preprocessing_function=get_random_eraser(v_l=0, v_h=0.5))\n",
    "\n",
    "def getAccuracy(y_result, y_test):\n",
    "    correct_amount =0 \n",
    "    for i, result in enumerate(y_result):\n",
    "        if result == y_test[i]:\n",
    "            correct_amount+=1\n",
    "    return correct_amount/len(y_test)\n",
    "\n",
    "def getOptimizer(type, learningRate):\n",
    "    match type:\n",
    "        case OptimizerType.Adam:\n",
    "            return Adam(learning_rate=learningRate)\n",
    "        case OptimizerType.Sgd:\n",
    "            return SGD(learning_rate=learningRate)\n",
    "        case OptimizerType.Lion:\n",
    "            return Lion(learning_rate=learningRate)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arrays containing different hyper-parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'Dataset/train'\n",
    "valid_dir = 'Dataset/valid'\n",
    "test_dir = 'Dataset/test'\n",
    "image_size = (32, 32)\n",
    "# training process\n",
    "# batchSizes =[64,128,256]\n",
    "batchSizes = [64]\n",
    "#learningRates = [0.001, 0.01,0.1]\n",
    "learningRates=[0.1]\n",
    "#numberOfEpochs =[5,10,15]\n",
    "numberOfEpochs=[5]\n",
    "\n",
    "# regularization\n",
    "# TODO: I guess we should add more variants of these\n",
    "#regularizers = [{\"type\":RegularizationType.NoRegularization,\"value\":0},{\"type\":RegularizationType.WeightDecay,\"value\":0.5},{\"type\":RegularizationType.L2,\"value\":0.01}]\n",
    "regularizers=[{\"type\":RegularizationType.WeightDecay,\"value\":0.5}]\n",
    "\n",
    "# augmentation\n",
    "# augmentationTechniques =[AugmentationTechnique.NoAugmentation,AugmentationTechnique.Rotation,AugmentationTechnique.Flipping,AugmentationTechnique.Contrast,AugmentationTechnique.Brightness,AugmentationTechnique.RandomErasing]\n",
    "augmentationTechniques=[AugmentationTechnique.RandomErasing]\n",
    "# seeds = [123,42,9]\n",
    "seeds = [123]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main pipeline loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performExperiment(modelType):\n",
    "    # TODO: Refactor to waterfall like approach\n",
    "    results = []\n",
    "    valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    for batchSize in batchSizes:\n",
    "        for learningRate in learningRates:\n",
    "            for epochNumber in numberOfEpochs:\n",
    "                for augmentation in augmentationTechniques:\n",
    "                    for regularizer in regularizers:\n",
    "                        for seed in seeds:\n",
    "                            set_seed(seed)\n",
    "\n",
    "                            train_augmented_data_generator = augumentData(technique=augmentation,seed=seeds[0])\n",
    "\n",
    "                            train_generator = train_augmented_data_generator.flow_from_directory(\n",
    "                                train_dir,\n",
    "                                target_size=image_size,\n",
    "                                batch_size=batchSize,\n",
    "                                class_mode='categorical'\n",
    "                            )\n",
    "\n",
    "                            valid_generator = valid_datagen.flow_from_directory(\n",
    "                                valid_dir,\n",
    "                                target_size=image_size,\n",
    "                                batch_size=batchSize,\n",
    "                                class_mode='categorical'\n",
    "                            )\n",
    "\n",
    "                            test_generator = valid_datagen.flow_from_directory(\n",
    "                                test_dir,\n",
    "                                target_size=image_size,\n",
    "                                batch_size=batchSize,\n",
    "                                class_mode='categorical'\n",
    "                            )\n",
    "                            # create the model\n",
    "\n",
    "                            model = createModel(modelType=modelType,regularizer=regularizer,optimizer=getOptimizer(type=OptimizerType.Sgd,learningRate=learningRate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "                            # train the model \n",
    "                            model.fit(train_data_generator=train_generator,batch_size=batchSize,epochs=epochNumber,valid_data_generator=valid_generator)\n",
    "\n",
    "                            # get accuracy\n",
    "                            y_pred = model.predict(test_generator)\n",
    "                            # Convert probabilities to class labels\n",
    "                            predicted_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "                            # Get true class labels\n",
    "                            true_classes = test_generator.classes\n",
    "                            accuracy = getAccuracy(predicted_classes,true_classes)\n",
    "\n",
    "                            # append to results\n",
    "                            # It's probably easier to create a simple 2d array and then transform it to a dataframe\n",
    "                            results.append([accuracy, augmentation,batchSize,learningRate,epochNumber])\n",
    "                            # results.append({'accuracy':accuracy,'augmentation': augmentation,'batchSize':batchSize,'learningRate':learningRate,'numberOfEpochs':epochNumber})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 144000 images belonging to 10 classes.\n",
      "Found 36000 images belonging to 10 classes.\n",
      "Found 90000 images belonging to 10 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prusak.patryk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\applications\\mobilenet_v3.py:512: UserWarning: `input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  return MobileNetV3(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prusak.patryk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 2s/step - accuracy: 0.1312 - loss: 3.3051 - val_accuracy: 0.1006 - val_loss: 2.9658\n",
      "Epoch 2/5\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 2s/step - accuracy: 0.1674 - loss: 2.7242 - val_accuracy: 0.0950 - val_loss: 2.7898\n",
      "Epoch 3/5\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 2s/step - accuracy: 0.1699 - loss: 2.6327 - val_accuracy: 0.0989 - val_loss: 2.4239\n",
      "Epoch 4/5\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 2s/step - accuracy: 0.1672 - loss: 2.6094 - val_accuracy: 0.0989 - val_loss: 2.5790\n",
      "Epoch 5/5\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 1s/step - accuracy: 0.1708 - loss: 2.5273 - val_accuracy: 0.1001 - val_loss: 2.5137\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1204s\u001b[0m 855ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.09956666666666666, <AugmentationTechnique.RandomErasing: 5>, 64, 0.1, 5]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performExperiment(modelType=ModelType.MobileNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNUSED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline checking each combination of parameters (with 3 values for each parameter it should take approx 48h on google colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performExperiment(modelType):\n",
    "    results = []\n",
    "    valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    for batchSize in batchSizes:\n",
    "        for learningRate in learningRates:\n",
    "            for epochNumber in numberOfEpochs:\n",
    "                for augmentation in augmentationTechniques:\n",
    "                    for regularizer in regularizers:\n",
    "                        for seed in seeds:\n",
    "                            set_seed(seed)\n",
    "\n",
    "                            train_augmented_data_generator = augumentData(technique=augmentation,seed=seeds[0])\n",
    "\n",
    "                            train_generator = train_augmented_data_generator.flow_from_directory(\n",
    "                                train_dir,\n",
    "                                target_size=image_size,\n",
    "                                batch_size=batchSize,\n",
    "                                class_mode='categorical'\n",
    "                            )\n",
    "\n",
    "                            valid_generator = valid_datagen.flow_from_directory(\n",
    "                                valid_dir,\n",
    "                                target_size=image_size,\n",
    "                                batch_size=batchSize,\n",
    "                                class_mode='categorical'\n",
    "                            )\n",
    "\n",
    "                            test_generator = valid_datagen.flow_from_directory(\n",
    "                                test_dir,\n",
    "                                target_size=image_size,\n",
    "                                batch_size=batchSize,\n",
    "                                class_mode='categorical'\n",
    "                            )\n",
    "                            # create the model\n",
    "\n",
    "                            model = createModel(modelType=modelType,regularizer=regularizer,optimizer=getOptimizer(type=OptimizerType.Sgd,learningRate=learningRate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "                            # train the model \n",
    "                            model.fit(train_data_generator=train_generator,batch_size=batchSize,epochs=epochNumber,valid_data_generator=valid_generator)\n",
    "\n",
    "                            # get accuracy\n",
    "                            y_pred = model.predict(test_generator)\n",
    "                            # Convert probabilities to class labels\n",
    "                            predicted_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "                            # Get true class labels\n",
    "                            true_classes = test_generator.classes\n",
    "                            accuracy = getAccuracy(predicted_classes,true_classes)\n",
    "\n",
    "                            # append to results\n",
    "                            # It's probably easier to create a simple 2d array and then transform it to a dataframe\n",
    "                            results.append([accuracy, augmentation,batchSize,learningRate,epochNumber])\n",
    "                            # results.append({'accuracy':accuracy,'augmentation': augmentation,'batchSize':batchSize,'learningRate':learningRate,'numberOfEpochs':epochNumber})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class HyperParameter(Enum):\n",
    "    BatchSize=1\n",
    "    LearningRate =2\n",
    "    NumberOfEpochs =3\n",
    "\n",
    "class HyperParameters:\n",
    "    def __init__(self, batchSizes, learningRates, numberOfEpochs):\n",
    "        self.batchSizes=batchSizes\n",
    "        self.learningRates=learningRates\n",
    "        self.numberOfEpochs=numberOfEpochs\n",
    "        self.currentIndex =0\n",
    "    \n",
    "    def getNextHyperParameter(self):\n",
    "        if self.currentIndex<len(self.batchSizes):\n",
    "            return self.batchSizes[self.currentIndex], HyperParameter.BatchSize\n",
    "        elif self.currentIndex<(len(self.batchSizes+self.learningRates)):\n",
    "            return self.learningRates[self.currentIndex-len(self.batchSizes)], HyperParameter.LearningRate\n",
    "        elif self.currentIndex<(len(self.batchSizes)+len(self.learningRates)+len(self.numberOfEpochs)):\n",
    "            return self.num"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
