{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3y86120l1hWf"
      },
      "source": [
        "3. Prepare the whole pipeline\n",
        "    1. Data augmentation\n",
        "        - First take the unaugmented original dataset and proceed\n",
        "        -  Augment the data with a predefined seed for each of the following techniques: rotation, flipping, contrast, brightness change, random erasing\n",
        "    2. Choose hyperparameters\n",
        "        - For the hyper-parameters related to the training process we chose batch size, learning rate, and number of epochs\n",
        "        - For the hyper-parameters related to Regularization we decided to use L2 Regularization (Weight Decay) and Dropout Rate\n",
        "    3. Train each of the prepared models on the augmented dataset for a chosen augmentation technique\n",
        "    4. Test and collect data regarding modelsâ€™ performance on the augmented dataset\n",
        "    5. Repeat several times (>=3) from 3.\n",
        "    6. Choose different values for hyperparameters and start from 3.\n",
        "    7. Choose the next augmentation technique and start from 2.\n",
        "    8. Repeat the process starting from 1. several times (>=3) with a different seed each time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rp54A3Vy6QcV",
        "outputId": "2ab5c148-04d0-40c5-fccd-5e491391e696"
      },
      "outputs": [],
      "source": [
        "train_dir = '/content/dataset/train'\n",
        "valid_dir = '/content/dataset/valid'\n",
        "test_dir = '/content/dataset/test'\n",
        "image_size = (32, 32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ICPEqaRu1hWj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from enum import Enum\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "import csv\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Lion\n",
        "from keras.regularizers import L2\n",
        "\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, BatchNormalization, Dropout, RandomRotation, RandomContrast, RandomBrightness, RandomFlip\n",
        "from efficientnet.tfkeras import EfficientNetB0\n",
        "from keras_cv.layers import RandomCutout\n",
        "from tensorflow.keras.applications import MobileNetV3Large\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "athIxi1I1hWm"
      },
      "source": [
        "## Move 5400 images from each class from valid to train\n",
        " - There is a safety check, if it has been already done it won't do it again :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "hmrpe5e11hWn"
      },
      "outputs": [],
      "source": [
        "for subdir, dirs, files in os.walk(valid_dir):\n",
        "    if subdir != valid_dir:\n",
        "        for subsubdir, subdirs, files in os.walk(subdir):\n",
        "            if len(files) < 5400:\n",
        "                break;\n",
        "            for i in range(5400):\n",
        "                os.rename(os.path.join(os.path.join(valid_dir,os.path.basename(subsubdir)),files[i]), os.path.join(os.path.join(train_dir,os.path.basename(subsubdir)),files[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41WOg_u_1hWo"
      },
      "source": [
        "## Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ldSMOKt1hWp"
      },
      "source": [
        "Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ixv4Rtgv1hWp"
      },
      "outputs": [],
      "source": [
        "class AugmentationTechnique(Enum):\n",
        "    NoAugmentation = 0\n",
        "    Rotation = 1\n",
        "    Flipping = 2\n",
        "    Contrast = 3\n",
        "    Brightness = 4\n",
        "    RandomErasing = 5\n",
        "\n",
        "class ModelType(Enum):\n",
        "    MobileNet =1\n",
        "    EfficientNet = 2\n",
        "\n",
        "class OptimizerType(Enum):\n",
        "    Adam = 1\n",
        "    Sgd=2\n",
        "    Lion=3\n",
        "class RegularizationType(Enum):\n",
        "    NoRegularization = 1\n",
        "    L2 =2\n",
        "    WeightDecay=3\n",
        "def getRegularizer(regularizerType,value):\n",
        "    match regularizerType:\n",
        "        case RegularizationType.L2:\n",
        "            return L2(value)\n",
        "def getDenseLayer(model,regularization, activation,nodes):\n",
        "        match regularization['type']:\n",
        "            case RegularizationType.WeightDecay:\n",
        "                model.add(Dropout(rate=regularization['value']))\n",
        "                model.add(Dense(nodes, activation=activation))\n",
        "            case RegularizationType.NoRegularization:\n",
        "                model.add(Dense(nodes, activation=activation))\n",
        "            case RegularizationType.L2:\n",
        "                model.add(Dense(nodes, activation=activation, kernel_regularizer=getRegularizer(type=regularization['type'],value=regularization['value'])))\n",
        "\n",
        "def getAugmentationLayer(model,technique, seed):\n",
        "    match technique:\n",
        "        case AugmentationTechnique.NoAugmentation:\n",
        "            return;\n",
        "        case AugmentationTechnique.Rotation:\n",
        "            model.add(RandomRotation(factor=(-0.2, 0.3),seed=seed))\n",
        "        case AugmentationTechnique.Flipping:\n",
        "            model.add(RandomFlip(mode=\"horizontal_and_vertical\", seed=seed))\n",
        "        case AugmentationTechnique.Brightness:\n",
        "            model.add(RandomBrightness(factor=(-0.4,0.4),seed=seed))\n",
        "        case AugmentationTechnique.Contrast:\n",
        "            model.add(RandomContrast(factor=0.4,seed=seed))\n",
        "        case AugmentationTechnique.RandomErasing:\n",
        "            model.add(RandomCutout(height_factor=0.5,width_factor=0.5,seed=seed))\n",
        "\n",
        "\n",
        "class Model:\n",
        "    def fit(self,batch_size,epochs,train_data_generator,valid_data_generator):\n",
        "        pass\n",
        "    def predict(self,test_data_generator):\n",
        "        pass\n",
        "    def __init__(self,optimizer,loss,metrics):\n",
        "        pass\n",
        "\n",
        "# TODO: implement 3rd model\n",
        "    # TODO: How should we handle Droput and L2, should it be applied to each Dense layer?\n",
        "class CustomMobileNetModel(Model):\n",
        "    def __init__(self,optimizer,loss,metrics,regularizer, seed):\n",
        "        # Load MobileNetV3Large without the top classification layer\n",
        "        base_model = MobileNetV3Large(include_top=False, weights='imagenet', input_shape=(32, 32, 3))\n",
        "\n",
        "        # Freeze the base model layers\n",
        "        base_model.trainable = False\n",
        "\n",
        "        # Add additional layers on top of MobileNetV3Large\n",
        "        model = tf.keras.Sequential()\n",
        "        model.add(base_model)\n",
        "        model.add(GlobalAveragePooling2D())\n",
        "\n",
        "        getDenseLayer(model=model,regularization=regularizer,activation='relu',nodes=1024)\n",
        "        model.add(BatchNormalization())\n",
        "        getDenseLayer(model=model,regularization=regularizer,activation='relu',nodes=512)\n",
        "        model.add(BatchNormalization())\n",
        "        getDenseLayer(model=model,regularization=regularizer,activation='softmax',nodes=10)\n",
        "            # Output layer with size 10 for classification\n",
        "        # Compile the model\n",
        "        model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "        self.model=model\n",
        "\n",
        "    def fit(self,batch_size,epochs,train_data_generator,valid_data_generator):\n",
        "        self.model.fit(\n",
        "        train_data_generator,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        validation_data=valid_data_generator\n",
        "        )\n",
        "    def predict(self,test_data_generator):\n",
        "        return self.model.predict(test_data_generator ,steps=len(test_data_generator))\n",
        "\n",
        "class CustomEfficientNetModel(Model):\n",
        "    def __init__(self,optimizer,loss,metrics, regularizer,seed):\n",
        "        # Load MobileNetV3Large without the top classification layer\n",
        "        base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
        "\n",
        "        # Freeze the base model layers\n",
        "        base_model.trainable = False\n",
        "        # Add additional layers on top of EfficientNet\n",
        "        model = tf.keras.Sequential()\n",
        "        model.add(base_model)\n",
        "        model.add(GlobalAveragePooling2D())\n",
        "        getDenseLayer(model=model,regularization=regularizer,activation='relu',nodes=1024)\n",
        "        model.add(BatchNormalization())\n",
        "        getDenseLayer(model=model,regularization=regularizer,activation='relu',nodes=512)\n",
        "        model.add(BatchNormalization())\n",
        "        getDenseLayer(model=model,regularization=regularizer,activation='softmax',nodes=10)\n",
        "            # Output layer with size 10 for classification\n",
        "        # Compile the model\n",
        "        model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "        self.model=model\n",
        "    def fit(self,batch_size,epochs,train_data_generator,valid_data_generator):\n",
        "        self.model.fit(\n",
        "        train_data_generator,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        validation_data=valid_data_generator,\n",
        "        )\n",
        "    def predict(self,test_data_generator):\n",
        "        return self.model.predict(test_data_generator)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jWYmdb71hWr"
      },
      "source": [
        "Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "AJx_kSx0FTPV"
      },
      "outputs": [],
      "source": [
        "# https://github.com/yu4u/cutout-random-erasing\n",
        "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
        "    def eraser(input_img):\n",
        "        if input_img.ndim == 3:\n",
        "            img_h, img_w, img_c = input_img.shape\n",
        "        elif input_img.ndim == 2:\n",
        "            img_h, img_w = input_img.shape\n",
        "\n",
        "        p_1 = np.random.rand()\n",
        "\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "\n",
        "        while True:\n",
        "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "            r = np.random.uniform(r_1, r_2)\n",
        "            w = int(np.sqrt(s / r))\n",
        "            h = int(np.sqrt(s * r))\n",
        "            left = np.random.randint(0, img_w)\n",
        "            top = np.random.randint(0, img_h)\n",
        "\n",
        "            if left + w <= img_w and top + h <= img_h:\n",
        "                break\n",
        "\n",
        "        if pixel_level:\n",
        "            if input_img.ndim == 3:\n",
        "                c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "            if input_img.ndim == 2:\n",
        "                c = np.random.uniform(v_l, v_h, (h, w))\n",
        "        else:\n",
        "            c = np.random.uniform(v_l, v_h)\n",
        "\n",
        "        input_img[top:top + h, left:left + w] = c\n",
        "\n",
        "        return input_img\n",
        "\n",
        "    return eraser\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Fzl_rEQ-1hWr"
      },
      "outputs": [],
      "source": [
        "\n",
        "def createModel(modelType, optimizer, loss, metrics, regularizer)->Model:\n",
        "    match modelType:\n",
        "        case ModelType.MobileNet:\n",
        "            return CustomMobileNetModel(optimizer=optimizer,loss=loss,metrics=metrics,regularizer=regularizer)\n",
        "        case ModelType.EfficientNet:\n",
        "            return CustomEfficientNetModel(optimizer=optimizer,loss=loss,metrics=metrics,regularizer=regularizer)\n",
        "\n",
        "def set_seed(seed=0):\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ['TF_DETERMINISTIC_OPS'] = \"1\"\n",
        "    os.environ['TF_CUDNN_DETERMINISM'] = \"1\"\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "def getContrastChange():\n",
        "    def contrastChange(input_img):\n",
        "        contrast = np.randint(0,100)\n",
        "        f = 131*(contrast + 127)/(127*(131-contrast))\n",
        "        alpha_c = f\n",
        "        gamma_c = 127*(1-f)\n",
        "\n",
        "        input_img = cv2.addWeighted(input_img, alpha_c, input_img, 0, gamma_c)\n",
        "        return input_img\n",
        "    return contrastChange\n",
        "\n",
        "\n",
        "def augumentData(technique, seed):\n",
        "    match technique:\n",
        "        case AugmentationTechnique.NoAugmentation:\n",
        "            return ImageDataGenerator()\n",
        "        case AugmentationTechnique.Rotation:\n",
        "            return ImageDataGenerator(rotation_range=20)\n",
        "        case AugmentationTechnique.Flipping:\n",
        "            return ImageDataGenerator(horizontal_flip=True,vertical_flip=True)\n",
        "        case AugmentationTechnique.Brightness:\n",
        "            return ImageDataGenerator(brightness_range=[0.2,1.8])\n",
        "        case AugmentationTechnique.Contrast:\n",
        "            return ImageDataGenerator(preprocessing_function=getContrastChange())\n",
        "        case AugmentationTechnique.RandomErasing:\n",
        "            return ImageDataGenerator(preprocessing_function=get_random_eraser(v_l=0, v_h=0.5))\n",
        "\n",
        "def getAccuracy(y_result, y_test):\n",
        "    correct_amount =0\n",
        "    for i, result in enumerate(y_result):\n",
        "        if result == y_test[i]:\n",
        "            correct_amount+=1\n",
        "    return correct_amount/len(y_test)\n",
        "\n",
        "def getOptimizer(type, learningRate):\n",
        "    match type:\n",
        "        case OptimizerType.Adam:\n",
        "            return Adam(learning_rate=learningRate)\n",
        "        case OptimizerType.Sgd:\n",
        "            return SGD(learning_rate=learningRate)\n",
        "        case OptimizerType.Lion:\n",
        "            return Lion(learning_rate=learningRate)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "im2wO1a91hWs"
      },
      "source": [
        "# Arrays containing different hyper-parameter values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-zUAYY91hWt"
      },
      "source": [
        "Main pipeline loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "iwNcho0sXsOH"
      },
      "outputs": [],
      "source": [
        "def performSingleExperiment(modelType, batchSize, epochNumber, augmentation, regularizer, learningRate, seed):\n",
        "    set_seed(seed)\n",
        "    train_augmented_data_generator = augumentData(technique=augmentation,seed=seed)\n",
        "    valid_datagen = ImageDataGenerator(rescale=1./255)\n",
        "    train_generator = train_augmented_data_generator.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=image_size,\n",
        "        batch_size=batchSize,\n",
        "        class_mode='categorical'\n",
        "    )\n",
        "\n",
        "    valid_generator = valid_datagen.flow_from_directory(\n",
        "        valid_dir,\n",
        "        target_size=image_size,\n",
        "        batch_size=batchSize,\n",
        "        class_mode='categorical'\n",
        "    )\n",
        "\n",
        "    test_generator = valid_datagen.flow_from_directory(\n",
        "        test_dir,\n",
        "        target_size=image_size,\n",
        "        batch_size=batchSize,\n",
        "        class_mode='categorical'\n",
        "    )\n",
        "    # create the model\n",
        "\n",
        "    model = createModel(modelType=modelType,regularizer=regularizer,optimizer=getOptimizer(type=OptimizerType.Sgd,learningRate=learningRate), loss='categorical_crossentropy', metrics=['accuracy'],seed=seed)\n",
        "    # train the model\n",
        "    model.fit(train_data_generator=train_generator,batch_size=batchSize,epochs=epochNumber,valid_data_generator=valid_generator)\n",
        "\n",
        "    # get accuracy\n",
        "    y_pred = model.predict(test_generator)\n",
        "    # Convert probabilities to class labels\n",
        "    predicted_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "    # Get true class labels\n",
        "    true_classes = test_generator.classes\n",
        "    accuracy = getAccuracy(predicted_classes,true_classes)\n",
        "    return [accuracy, augmentation,batchSize,learningRate,epochNumber]\n",
        "\n",
        "\n",
        "def performExperiment(batchSizes, learningRates, numberOfEpochs, augmentationTechniques, regularizers,modelType,seeds):\n",
        "    results = []\n",
        "    accuracy=0\n",
        "    currentBestBatchSize = batchSizes[0]\n",
        "    currentBestLearningRate = learningRates[0]\n",
        "    currentBestNumberOfEpochs = numberOfEpochs[0]\n",
        "    currentBestAugmentation =augmentationTechniques[0]\n",
        "    currentBestRegularizer = regularizers[0\n",
        "                                          ]\n",
        "    for batchSize in batchSizes:\n",
        "        for seed in seeds:\n",
        "            results.append(performSingleExperiment(modelType=modelType,learningRate=currentBestLearningRate,batchSize=batchSize,epochNumber=currentBestNumberOfEpochs,augmentation=currentBestAugmentation,regularizer=currentBestRegularizer,seed=seed))\n",
        "            if results[len(results)-1][0]>accuracy:\n",
        "                currentBestBatchSize=batchSize\n",
        "                accuracy=results[len(results)-1][0]\n",
        "    accuracy=0\n",
        "    for learningRate in learningRates:\n",
        "        for seed in seeds:\n",
        "            results.append(performSingleExperiment(modelType=modelType,learningRate=learningRate,batchSize=currentBestBatchSize,epochNumber=currentBestNumberOfEpochs,augmentation=currentBestAugmentation,regularizer=currentBestRegularizer,seed=seed))\n",
        "            if results[len(results)-1][0]>accuracy:\n",
        "                currentBestLearningRate=learningRate\n",
        "                accuracy=results[len(results)-1][0]\n",
        "    accuracy=0\n",
        "    for epochNumber in numberOfEpochs:\n",
        "        for seed in seeds:\n",
        "            results.append(performSingleExperiment(modelType=modelType,learningRate=currentBestLearningRate,batchSize=currentBestBatchSize,epochNumber=epochNumber,augmentation=currentBestAugmentation,regularizer=currentBestRegularizer,seed=seed))\n",
        "            if results[len(results)-1][0]>accuracy:\n",
        "                currentBestNumberOfEpochs=epochNumber\n",
        "                accuracy=results[len(results)-1][0]\n",
        "    accuracy=0\n",
        "    for augmentation in augmentationTechniques:\n",
        "        for seed in seeds:\n",
        "            results.append(performSingleExperiment(modelType=modelType,learningRate=currentBestLearningRate,batchSize=currentBestBatchSize,epochNumber=currentBestNumberOfEpochs,augmentation=augmentation,regularizer=currentBestRegularizer,seed=seed))\n",
        "            if results[len(results)-1][0]>accuracy:\n",
        "                currentBestAugmentation=augmentation\n",
        "                accuracy=results[len(results)-1][0]\n",
        "    accuracy=0\n",
        "    for regularizer in regularizers:\n",
        "        for seed in seeds:\n",
        "            results.append(performSingleExperiment(modelType=modelType,learningRate=currentBestLearningRate,batchSize=currentBestBatchSize,epochNumber=currentBestNumberOfEpochs,augmentation=currentBestAugmentation,regularizer=regularizer,seed=seed))\n",
        "            if results[len(results)-1][0]>accuracy:\n",
        "                currentBestRegularizer=regularizer\n",
        "                accuracy=results[len(results)-1][0]\n",
        "\n",
        "\n",
        "    return results, [accuracy,currentBestBatchSize, currentBestLearningRate,currentBestNumberOfEpochs, currentBestAugmentation,currentBestRegularizer]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmHrDdcJ2xVN"
      },
      "source": [
        "# Arrays containing different hyper-parameter values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "LtzMgiW4e8zz"
      },
      "outputs": [],
      "source": [
        "\n",
        "# training process\n",
        "#batchSizes =[64,128,256]\n",
        "batchSizes = [512]\n",
        "#learningRates = [0.001, 0.01,0.1]\n",
        "learningRates=[0.1]\n",
        "#numberOfEpochs =[5,10,15]\n",
        "numberOfEpochs=[10]\n",
        "\n",
        "# regularization\n",
        "# TODO: I guess we should add more variants of these\n",
        "#regularizers = [{\"type\":RegularizationType.NoRegularization,\"value\":0},{\"type\":RegularizationType.WeightDecay,\"value\":0.5},{\"type\":RegularizationType.L2,\"value\":0.01}]\n",
        "regularizers=[{\"type\":RegularizationType.NoRegularization,\"value\":0}]\n",
        "\n",
        "# augmentation\n",
        "#augmentationTechniques =[AugmentationTechnique.NoAugmentation,AugmentationTechnique.Rotation,AugmentationTechnique.Flipping,AugmentationTechnique.Contrast,AugmentationTechnique.Brightness,AugmentationTechnique.RandomErasing]\n",
        "augmentationTechniques=[AugmentationTechnique.Flipping]\n",
        "#seeds = [123,42,9]\n",
        "seeds = [123]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "DEDdhh5yQv0s"
      },
      "outputs": [],
      "source": [
        "results=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "Z7tLBKq21hWv",
        "outputId": "f833ead0-6df6-4d05-f230-4d5f5b1c6e47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 144000 images belonging to 10 classes.\n",
            "Found 36000 images belonging to 10 classes.\n",
            "Found 90000 images belonging to 10 classes.\n",
            "Epoch 1/10\n",
            " 26/282 [=>............................] - ETA: 55s - loss: 2.7161 - accuracy: 0.1090"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-6b34e6dc41b1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperformExperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelType\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModelType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEfficientNet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatchSizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatchSizes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearningRates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearningRates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnumberOfEpochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumberOfEpochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maugmentationTechniques\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maugmentationTechniques\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mregularizers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregularizers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-051976790956>\u001b[0m in \u001b[0;36mperformExperiment\u001b[0;34m(batchSizes, learningRates, numberOfEpochs, augmentationTechniques, regularizers, modelType, seeds)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatchSize\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatchSizes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseeds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperformSingleExperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelType\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodelType\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearningRate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurrentBestLearningRate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochNumber\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurrentBestNumberOfEpochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maugmentation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurrentBestAugmentation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mregularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurrentBestRegularizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0mcurrentBestBatchSize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-051976790956>\u001b[0m in \u001b[0;36mperformSingleExperiment\u001b[0;34m(modelType, batchSize, epochNumber, augmentation, regularizer, learningRate, seed)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreateModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelType\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodelType\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mregularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregularizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgetOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOptimizerType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSgd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearningRate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearningRate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_generator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochNumber\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_data_generator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# get accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-62048e6bf1c1>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, batch_size, epochs, train_data_generator, valid_data_generator)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_data_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_data_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         self.model.fit(\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0mtrain_data_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "result, best = performExperiment(modelType=ModelType.EfficientNet,batchSizes=batchSizes,learningRates=learningRates,numberOfEpochs=numberOfEpochs,augmentationTechniques=augmentationTechniques,regularizers=regularizers,seeds=seeds)\n",
        "results.append(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "z-ofA_IXNW6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[2, 3], [1, 4]]\n",
            "[{<AugmentationTechnique.Brightness: 4>, 5}]\n"
          ]
        }
      ],
      "source": [
        "print(results)\n",
        "print(best)\n",
        "\n",
        "with open(\"results.csv\",\"w+\") as my_csv:\n",
        "    csvWriter = csv.writer(my_csv,delimiter=',')\n",
        "    csvWriter.writerows(results)\n",
        "\n",
        "with open(\"best.csv\",\"w+\") as my_csv:\n",
        "    csvWriter = csv.writer(my_csv,delimiter=',')\n",
        "    csvWriter.writerows(best)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeOnfA811hWw"
      },
      "source": [
        "# UNUSED"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnX-HCvtXmEs"
      },
      "source": [
        "# Pipeline checking each combination of parameters (with 3 values for each parameter it should take approx 48h on google colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoDn4uo_1hWu"
      },
      "outputs": [],
      "source": [
        "def performExperiment(modelType):\n",
        "    results = []\n",
        "    valid_datagen = ImageDataGenerator(rescale=1./255)\n",
        "    for batchSize in batchSizes:\n",
        "        for learningRate in learningRates:\n",
        "            for epochNumber in numberOfEpochs:\n",
        "                for augmentation in augmentationTechniques:\n",
        "                    for regularizer in regularizers:\n",
        "                        for seed in seeds:\n",
        "                            set_seed(seed)\n",
        "\n",
        "                            train_augmented_data_generator = augumentData(technique=augmentation,seed=seeds[0])\n",
        "\n",
        "                            train_generator = train_augmented_data_generator.flow_from_directory(\n",
        "                                train_dir,\n",
        "                                target_size=image_size,\n",
        "                                batch_size=batchSize,\n",
        "                                class_mode='categorical'\n",
        "                            )\n",
        "\n",
        "                            valid_generator = valid_datagen.flow_from_directory(\n",
        "                                valid_dir,\n",
        "                                target_size=image_size,\n",
        "                                batch_size=batchSize,\n",
        "                                class_mode='categorical'\n",
        "                            )\n",
        "\n",
        "                            test_generator = valid_datagen.flow_from_directory(\n",
        "                                test_dir,\n",
        "                                target_size=image_size,\n",
        "                                batch_size=batchSize,\n",
        "                                class_mode='categorical'\n",
        "                            )\n",
        "                            # create the model\n",
        "\n",
        "                            model = createModel(modelType=modelType,regularizer=regularizer,optimizer=getOptimizer(type=OptimizerType.Sgd,learningRate=learningRate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "                            # train the model\n",
        "                            model.fit(train_data_generator=train_generator,batch_size=batchSize,epochs=epochNumber,valid_data_generator=valid_generator)\n",
        "\n",
        "                            # get accuracy\n",
        "                            y_pred = model.predict(test_generator)\n",
        "                            # Convert probabilities to class labels\n",
        "                            predicted_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "                            # Get true class labels\n",
        "                            true_classes = test_generator.classes\n",
        "                            accuracy = getAccuracy(predicted_classes,true_classes)\n",
        "\n",
        "                            # append to results\n",
        "                            # It's probably easier to create a simple 2d array and then transform it to a dataframe\n",
        "                            results.append([accuracy, augmentation,batchSize,learningRate,epochNumber])\n",
        "                            # results.append({'accuracy':accuracy,'augmentation': augmentation,'batchSize':batchSize,'learningRate':learningRate,'numberOfEpochs':epochNumber})\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHAOCq8S1hWw"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class HyperParameter(Enum):\n",
        "    BatchSize=1\n",
        "    LearningRate =2\n",
        "    NumberOfEpochs =3\n",
        "\n",
        "class HyperParameters:\n",
        "    def __init__(self, batchSizes, learningRates, numberOfEpochs):\n",
        "        self.batchSizes=batchSizes\n",
        "        self.learningRates=learningRates\n",
        "        self.numberOfEpochs=numberOfEpochs\n",
        "        self.currentIndex =0\n",
        "\n",
        "    def getNextHyperParameter(self):\n",
        "        if self.currentIndex<len(self.batchSizes):\n",
        "            return self.batchSizes[self.currentIndex], HyperParameter.BatchSize\n",
        "        elif self.currentIndex<(len(self.batchSizes+self.learningRates)):\n",
        "            return self.learningRates[self.currentIndex-len(self.batchSizes)], HyperParameter.LearningRate\n",
        "        elif self.currentIndex<(len(self.batchSizes)+len(self.learningRates)+len(self.numberOfEpochs)):\n",
        "            return self.num"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
