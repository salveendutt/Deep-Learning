{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Prepare the whole pipeline\n",
    "    1. Data augmentation\n",
    "        - First take the unaugmented original dataset and proceed\n",
    "        -  Augment the data with a predefined seed for each of the following techniques: rotation, flipping, contrast, brightness change, random erasing\n",
    "    2. Choose hyperparameters\n",
    "        - For the hyper-parameters related to the training process we chose batch size, learning rate, and number of epochs\n",
    "        - For the hyper-parameters related to Regularization we decided to use L2 Regularization (Weight Decay) and Dropout Rate\n",
    "    3. Train each of the prepared models on the augmented dataset for a chosen augmentation technique\n",
    "    4. Test and collect data regarding models’ performance on the augmented dataset\n",
    "    5. Repeat several times (>=3) from 3.\n",
    "    6. Choose different values for hyperparameters and start from 3.\n",
    "    7. Choose the next augmentation technique and start from 2.\n",
    "    8. Repeat the process starting from 1. several times (>=3) with a different seed each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from enum import Enum\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam, SGD, Lion\n",
    "from keras.regularizers import L2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, BatchNormalization, Dropout\n",
    "from efficientnet.tfkeras import EfficientNetB0\n",
    "from tensorflow.keras.applications import MobileNetV3Large\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'Dataset/train'\n",
    "valid_dir = 'Dataset/valid'\n",
    "test_dir = 'Dataset/test'\n",
    "image_size = (32, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move 5400 images from each class from valid to train\n",
    " - There is a safety check, if it has been already done it won't do it again :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subdir, dirs, files in os.walk(valid_dir):\n",
    "    if subdir != valid_dir:\n",
    "        for subsubdir, subdirs, files in os.walk(subdir):\n",
    "            if len(files) < 5400:\n",
    "                break;\n",
    "            for i in range(5400):\n",
    "                os.rename(os.path.join(os.path.join(valid_dir,os.path.basename(subsubdir)),files[i]), os.path.join(os.path.join(train_dir,os.path.basename(subsubdir)),files[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentationTechnique(Enum):\n",
    "    NoAugmentation = 0\n",
    "    Rotation = 1\n",
    "    Flipping = 2\n",
    "    Contrast = 3\n",
    "    Brightness = 4\n",
    "    RandomErasing = 5\n",
    "\n",
    "class ModelType(Enum):\n",
    "    MobileNet =1\n",
    "    EfficientNet = 2\n",
    "\n",
    "class OptimizerType(Enum):\n",
    "    Adam = 1\n",
    "    Sgd=2\n",
    "    Lion=3\n",
    "class RegularizationType(Enum):\n",
    "    NoRegularization = 1\n",
    "    L2 =2\n",
    "    WeightDecay=3\n",
    "def getRegularizer(regularizerType,value):\n",
    "    match regularizerType:\n",
    "        case RegularizationType.L2:\n",
    "            return L2(value)\n",
    "def getDenseLayer(model,regularization, activation,nodes):\n",
    "        match regularization['type']:\n",
    "            case RegularizationType.WeightDecay:\n",
    "                model.add(Dropout(rate=regularization['value']))\n",
    "                model.add(Dense(nodes, activation=activation))\n",
    "            case RegularizationType.NoRegularization:\n",
    "                model.add(Dense(nodes, activation=activation))\n",
    "            case RegularizationType.L2:\n",
    "                model.add(Dense(nodes, activation=activation, kernel_regularizer=getRegularizer(type=regularization['type'],value=regularization['value'])))\n",
    "\n",
    "class Model:\n",
    "    def fit(self,batch_size,epochs,train_data_generator,valid_data_generator):\n",
    "        pass\n",
    "    def predict(self,test_data_generator):\n",
    "        pass\n",
    "    def __init__(self,optimizer,loss,metrics):\n",
    "        pass\n",
    "\n",
    "# TODO: implement 3rd model\n",
    "    # TODO: How should we handle Droput and L2, should it be applied to each Dense layer?\n",
    "class CustomMobileNetModel(Model):\n",
    "    def __init__(self,optimizer,loss,metrics,regularizer):\n",
    "        # Load MobileNetV3Large without the top classification layer\n",
    "        base_model = MobileNetV3Large(include_top=False, weights='imagenet', input_shape=(32, 32, 3))\n",
    "\n",
    "        # Freeze the base model layers\n",
    "        base_model.trainable = False\n",
    "\n",
    "        # Add additional layers on top of MobileNetV3Large\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(base_model)\n",
    "        model.add(GlobalAveragePooling2D())\n",
    "\n",
    "        getDenseLayer(model=model,regularization=regularizer,activation='relu',nodes=1024)\n",
    "        model.add(BatchNormalization())\n",
    "        getDenseLayer(model=model,regularization=regularizer,activation='relu',nodes=512)\n",
    "        model.add(BatchNormalization())\n",
    "        getDenseLayer(model=model,regularization=regularizer,activation='softmax',nodes=10)\n",
    "            # Output layer with size 10 for classification\n",
    "        # Compile the model\n",
    "        model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "        self.model=model\n",
    "\n",
    "    def fit(self,batch_size,epochs,train_data_generator,valid_data_generator):\n",
    "        self.model.fit(\n",
    "        train_data_generator,\n",
    "        steps_per_epoch=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=valid_data_generator,\n",
    "        validation_steps=batch_size\n",
    "        )\n",
    "    def predict(self,test_data_generator):\n",
    "        return self.model.predict(test_data_generator ,steps=len(test_data_generator))\n",
    "    \n",
    "class CustomEfficientNetModel(Model):\n",
    "    def __init__(self,optimizer,loss,metrics, regularizer):\n",
    "        # Load MobileNetV3Large without the top classification layer\n",
    "        base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
    "        \n",
    "        # Freeze the base model layers\n",
    "        base_model.trainable = False\n",
    "        # Add additional layers on top of EfficientNet\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(base_model)\n",
    "        model.add(GlobalAveragePooling2D())\n",
    "        getDenseLayer(model=model,regularization=regularizer,activation='relu',nodes=1024)\n",
    "        model.add(BatchNormalization())\n",
    "        getDenseLayer(model=model,regularization=regularizer,activation='relu',nodes=512)\n",
    "        model.add(BatchNormalization())\n",
    "        getDenseLayer(model=model,regularization=regularizer,activation='softmax',nodes=10)\n",
    "            # Output layer with size 10 for classification\n",
    "        # Compile the model\n",
    "        model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "        self.model=model\n",
    "    def fit(self,batch_size,epochs,train_data_generator,valid_data_generator):\n",
    "        self.model.fit(\n",
    "        train_data_generator,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=valid_data_generator,\n",
    "        )\n",
    "    def predict(self,test_data_generator):\n",
    "        return self.model.predict(test_data_generator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/yu4u/cutout-random-erasing\n",
    "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
    "    def eraser(input_img):\n",
    "        if input_img.ndim == 3:\n",
    "            img_h, img_w, img_c = input_img.shape\n",
    "        elif input_img.ndim == 2:\n",
    "            img_h, img_w = input_img.shape\n",
    "\n",
    "        p_1 = np.random.rand()\n",
    "\n",
    "        if p_1 > p:\n",
    "            return input_img\n",
    "\n",
    "        while True:\n",
    "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
    "            r = np.random.uniform(r_1, r_2)\n",
    "            w = int(np.sqrt(s / r))\n",
    "            h = int(np.sqrt(s * r))\n",
    "            left = np.random.randint(0, img_w)\n",
    "            top = np.random.randint(0, img_h)\n",
    "\n",
    "            if left + w <= img_w and top + h <= img_h:\n",
    "                break\n",
    "\n",
    "        if pixel_level:\n",
    "            if input_img.ndim == 3:\n",
    "                c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
    "            if input_img.ndim == 2:\n",
    "                c = np.random.uniform(v_l, v_h, (h, w))\n",
    "        else:\n",
    "            c = np.random.uniform(v_l, v_h)\n",
    "\n",
    "        input_img[top:top + h, left:left + w] = c\n",
    "\n",
    "        return input_img\n",
    "\n",
    "    return eraser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def createModel(modelType, optimizer, loss, metrics, regularizer)->Model:\n",
    "    match modelType:\n",
    "        case ModelType.MobileNet:\n",
    "            return CustomMobileNetModel(optimizer=optimizer,loss=loss,metrics=metrics,regularizer=regularizer)\n",
    "        case ModelType.EfficientNet:\n",
    "            return NotImplementedError\n",
    "\n",
    "def set_seed(seed=0):\n",
    "    np.random.seed(seed) \n",
    "    tf.random.set_seed(seed) \n",
    "    random.seed(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = \"1\"\n",
    "    os.environ['TF_CUDNN_DETERMINISM'] = \"1\"\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "def getContrastChange():\n",
    "    def contrastChange(input_img):\n",
    "        contrast = np.randint(0,100)\n",
    "        f = 131*(contrast + 127)/(127*(131-contrast))\n",
    "        alpha_c = f\n",
    "        gamma_c = 127*(1-f)\n",
    "\n",
    "        input_img = cv2.addWeighted(input_img, alpha_c, input_img, 0, gamma_c)\n",
    "        return input_img\n",
    "    return contrastChange\n",
    "\n",
    "\n",
    "def augumentData(technique, seed):\n",
    "    match technique:\n",
    "        case AugmentationTechnique.NoAugmentation:\n",
    "            return ImageDataGenerator()\n",
    "        case AugmentationTechnique.Rotation:\n",
    "            return ImageDataGenerator(rotation_range=20)\n",
    "        case AugmentationTechnique.Flipping:\n",
    "            return ImageDataGenerator(horizontal_flip=True,vertical_flip=True)\n",
    "        case AugmentationTechnique.Brightness:\n",
    "            return ImageDataGenerator(brightness_range=[0.2,1.8])\n",
    "        case AugmentationTechnique.Contrast:\n",
    "            return ImageDataGenerator(preprocessing_function=getContrastChange())\n",
    "        case AugmentationTechnique.RandomErasing:\n",
    "            return ImageDataGenerator(preprocessing_function=get_random_eraser(v_l=0, v_h=0.5))\n",
    "\n",
    "def getAccuracy(y_result, y_test):\n",
    "    correct_amount =0 \n",
    "    for i, result in enumerate(y_result):\n",
    "        if result == y_test[i]:\n",
    "            correct_amount+=1\n",
    "    return correct_amount/len(y_test)\n",
    "\n",
    "def getOptimizer(type, learningRate):\n",
    "    match type:\n",
    "        case OptimizerType.Adam:\n",
    "            return Adam(learning_rate=learningRate)\n",
    "        case OptimizerType.Sgd:\n",
    "            return SGD(learning_rate=learningRate)\n",
    "        case OptimizerType.Lion:\n",
    "            return Lion(learning_rate=learningRate)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main pipeline loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performSingleExperiment(modelType, batchSize, epochNumber, augmentation, regularizer, learningRate, seed):\n",
    "    set_seed(seed)\n",
    "    train_augmented_data_generator = augumentData(technique=augmentation,seed=seed)\n",
    "    valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    train_generator = train_augmented_data_generator.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=image_size,\n",
    "        batch_size=batchSize,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "\n",
    "    valid_generator = valid_datagen.flow_from_directory(\n",
    "        valid_dir,\n",
    "        target_size=image_size,\n",
    "        batch_size=batchSize,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "\n",
    "    test_generator = valid_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=image_size,\n",
    "        batch_size=batchSize,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "    # create the model\n",
    "\n",
    "    model = createModel(modelType=modelType,regularizer=regularizer,optimizer=getOptimizer(type=OptimizerType.Sgd,learningRate=learningRate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # train the model \n",
    "    model.fit(train_data_generator=train_generator,batch_size=batchSize,epochs=epochNumber,valid_data_generator=valid_generator)\n",
    "\n",
    "    # get accuracy\n",
    "    y_pred = model.predict(test_generator)\n",
    "    # Convert probabilities to class labels\n",
    "    predicted_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    # Get true class labels\n",
    "    true_classes = test_generator.classes\n",
    "    accuracy = getAccuracy(predicted_classes,true_classes)\n",
    "\n",
    "    return [accuracy, augmentation,batchSize,learningRate,epochNumber]\n",
    "\n",
    "\n",
    "def performExperiment(batchSizes, learningRates, numberOfEpochs, augmentationTechniques, regularizers,modelType,seeds):\n",
    "    results = []\n",
    "    accuracy=0\n",
    "    currentBestBatchSize = batchSizes[0]\n",
    "    currentBestLearningRate = learningRates[0]\n",
    "    currentBestNumberOfEpochs = numberOfEpochs[0]\n",
    "    currentBestAugmentation =augmentationTechniques[0]\n",
    "    currentBestRegularizer = regularizers[0\n",
    "                                          ]\n",
    "    for batchSize in batchSizes:\n",
    "        for seed in seeds:\n",
    "            results.append(performSingleExperiment(modelType=modelType,learningRate=currentBestLearningRate,batchSize=batchSize,epochNumber=currentBestNumberOfEpochs,augmentation=currentBestAugmentation,regularizer=currentBestRegularizer,seed=seed))\n",
    "            if results[len(results)-1][0]>accuracy:\n",
    "                currentBestBatchSize=batchSize\n",
    "                accuracy=results[len(results)-1][0]\n",
    "    accuracy=0\n",
    "    for learningRate in learningRates:\n",
    "        for seed in seeds:\n",
    "            results.append(performSingleExperiment(modelType=modelType,learningRate=learningRate,batchSize=currentBestBatchSize,epochNumber=currentBestNumberOfEpochs,augmentation=currentBestAugmentation,regularizer=currentBestRegularizer,seed=seed))\n",
    "            if results[len(results)-1][0]>accuracy:\n",
    "                currentBestLearningRate=learningRate\n",
    "                accuracy=results[len(results)-1][0]\n",
    "    accuracy=0\n",
    "    for epochNumber in numberOfEpochs:\n",
    "        for seed in seeds:\n",
    "            results.append(performSingleExperiment(modelType=modelType,learningRate=currentBestLearningRate,batchSize=currentBestBatchSize,epochNumber=epochNumber,augmentation=currentBestAugmentation,regularizer=currentBestRegularizer,seed=seed))\n",
    "            if results[len(results)-1][0]>accuracy:\n",
    "                currentBestNumberOfEpochs=epochNumber\n",
    "                accuracy=results[len(results)-1][0]\n",
    "    accuracy=0\n",
    "    for augmentation in augmentationTechniques:\n",
    "        for seed in seeds:\n",
    "            results.append(performSingleExperiment(modelType=modelType,learningRate=currentBestLearningRate,batchSize=currentBestBatchSize,epochNumber=currentBestNumberOfEpochs,augmentation=augmentation,regularizer=currentBestRegularizer,seed=seed))\n",
    "            if results[len(results)-1][0]>accuracy:\n",
    "                currentBestAugmentation=augmentation\n",
    "                accuracy=results[len(results)-1][0]\n",
    "    accuracy=0\n",
    "    for regularizer in regularizers:\n",
    "        for seed in seeds:\n",
    "            results.append(performSingleExperiment(modelType=modelType,learningRate=currentBestLearningRate,batchSize=currentBestBatchSize,epochNumber=currentBestNumberOfEpochs,augmentation=currentBestAugmentation,regularizer=regularizer,seed=seed))\n",
    "            if results[len(results)-1][0]>accuracy:\n",
    "                currentBestRegularizer=regularizer\n",
    "                accuracy=results[len(results)-1][0]\n",
    "\n",
    "                        \n",
    "    return results,[accuracy,currentBestBatchSize, currentBestLearningRate,currentBestNumberOfEpochs, currentBestAugmentation,currentBestRegularizer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arrays containing different hyper-parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# training process\n",
    "# batchSizes =[64,128,256]\n",
    "batchSizes = [64]\n",
    "#learningRates = [0.001, 0.01,0.1]\n",
    "learningRates=[0.1]\n",
    "#numberOfEpochs =[5,10,15]\n",
    "numberOfEpochs=[5]\n",
    "\n",
    "# regularization\n",
    "# TODO: I guess we should add more variants of these\n",
    "#regularizers = [{\"type\":RegularizationType.NoRegularization,\"value\":0},{\"type\":RegularizationType.WeightDecay,\"value\":0.5},{\"type\":RegularizationType.L2,\"value\":0.01}]\n",
    "regularizers=[{\"type\":RegularizationType.WeightDecay,\"value\":0.5}]\n",
    "\n",
    "# augmentation\n",
    "# augmentationTechniques =[AugmentationTechnique.NoAugmentation,AugmentationTechnique.Rotation,AugmentationTechnique.Flipping,AugmentationTechnique.Contrast,AugmentationTechnique.Brightness,AugmentationTechnique.RandomErasing]\n",
    "augmentationTechniques=[AugmentationTechnique.RandomErasing]\n",
    "# seeds = [123,42,9]\n",
    "seeds = [123]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 144000 images belonging to 10 classes.\n",
      "Found 36000 images belonging to 10 classes.\n",
      "Found 90000 images belonging to 10 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prusak.patryk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\applications\\mobilenet_v3.py:512: UserWarning: `input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  return MobileNetV3(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prusak.patryk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 310ms/step - accuracy: 0.1312 - loss: 3.3051 - val_accuracy: 0.1006 - val_loss: 2.9658\n",
      "Epoch 2/5\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 507ms/step - accuracy: 0.1674 - loss: 2.7242 - val_accuracy: 0.0950 - val_loss: 2.7898\n",
      "Epoch 3/5\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 2s/step - accuracy: 0.1699 - loss: 2.6327 - val_accuracy: 0.0989 - val_loss: 2.4239\n",
      "Epoch 4/5\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 2s/step - accuracy: 0.1672 - loss: 2.6094 - val_accuracy: 0.0989 - val_loss: 2.5790\n",
      "Epoch 5/5\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 2s/step - accuracy: 0.1708 - loss: 2.5273 - val_accuracy: 0.1001 - val_loss: 2.5137\n",
      "\u001b[1m 109/1407\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19:54\u001b[0m 921ms/step"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mperformExperiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodelType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mModelType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMobileNet\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatchSizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatchSizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlearningRates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearningRates\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnumberOfEpochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumberOfEpochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43maugmentationTechniques\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugmentationTechniques\u001b[49m\u001b[43m,\u001b[49m\u001b[43mregularizers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregularizers\u001b[49m\u001b[43m,\u001b[49m\u001b[43mseeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseeds\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 55\u001b[0m, in \u001b[0;36mperformExperiment\u001b[1;34m(batchSizes, learningRates, numberOfEpochs, augmentationTechniques, regularizers, modelType, seeds)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batchSize \u001b[38;5;129;01min\u001b[39;00m batchSizes:\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m seed \u001b[38;5;129;01min\u001b[39;00m seeds:\n\u001b[1;32m---> 55\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\u001b[43mperformSingleExperiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodelType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodelType\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlearningRate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurrentBestLearningRate\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochNumber\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurrentBestNumberOfEpochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43maugmentation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurrentBestAugmentation\u001b[49m\u001b[43m,\u001b[49m\u001b[43mregularizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurrentBestRegularizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     56\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m results[\u001b[38;5;28mlen\u001b[39m(results)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m>\u001b[39maccuracy:\n\u001b[0;32m     57\u001b[0m             currentBestBatchSize\u001b[38;5;241m=\u001b[39mbatchSize\n",
      "Cell \u001b[1;32mIn[12], line 32\u001b[0m, in \u001b[0;36mperformSingleExperiment\u001b[1;34m(modelType, batchSize, epochNumber, augmentation, regularizer, learningRate, seed)\u001b[0m\n\u001b[0;32m     29\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(train_data_generator\u001b[38;5;241m=\u001b[39mtrain_generator,batch_size\u001b[38;5;241m=\u001b[39mbatchSize,epochs\u001b[38;5;241m=\u001b[39mepochNumber,valid_data_generator\u001b[38;5;241m=\u001b[39mvalid_generator)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# get accuracy\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_generator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Convert probabilities to class labels\u001b[39;00m\n\u001b[0;32m     34\u001b[0m predicted_classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y_pred, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 77\u001b[0m, in \u001b[0;36mCustomMobileNetModel.predict\u001b[1;34m(self, test_data_generator)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m,test_data_generator):\n\u001b[1;32m---> 77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data_generator\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_data_generator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prusak.patryk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\prusak.patryk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:514\u001b[0m, in \u001b[0;36mTensorFlowTrainer.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    513\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_begin(step)\n\u001b[1;32m--> 514\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    515\u001b[0m     batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_function(data)\n\u001b[0;32m    516\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m append_to_outputs(batch_outputs, outputs)\n",
      "File \u001b[1;32mc:\\Users\\prusak.patryk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:494\u001b[0m, in \u001b[0;36mTensorFlowTrainer.predict.<locals>.get_data\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_execution):\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 494\u001b[0m         single_step_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mStopIteration\u001b[39;00m, tf\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mOutOfRangeError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__len__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    497\u001b[0m             \u001b[38;5;66;03m# Suppress the error when still have remaining data.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prusak.patryk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:809\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    807\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    808\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 809\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    810\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n\u001b[0;32m    811\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prusak.patryk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:772\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    769\u001b[0m \u001b[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[0;32m    770\u001b[0m \u001b[38;5;66;03m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecution_mode(context\u001b[38;5;241m.\u001b[39mSYNC):\n\u001b[1;32m--> 772\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator_get_next\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    773\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    774\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    775\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    777\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    778\u001b[0m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec\u001b[38;5;241m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prusak.patryk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3081\u001b[0m, in \u001b[0;36miterator_get_next\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   3079\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   3080\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3081\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3082\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIteratorGetNext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_types\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3083\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_shapes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3084\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   3085\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result=[]\n",
    "best = []\n",
    "result,best=performExperiment(modelType=ModelType.MobileNet,batchSizes=batchSizes,learningRates=learningRates,numberOfEpochs=numberOfEpochs,augmentationTechniques=augmentationTechniques,regularizers=regularizers,seeds=seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNUSED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline checking each combination of parameters (with 3 values for each parameter it should take approx 48h on google colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performExperiment(modelType):\n",
    "    results = []\n",
    "    valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    for batchSize in batchSizes:\n",
    "        for learningRate in learningRates:\n",
    "            for epochNumber in numberOfEpochs:\n",
    "                for augmentation in augmentationTechniques:\n",
    "                    for regularizer in regularizers:\n",
    "                        for seed in seeds:\n",
    "                            set_seed(seed)\n",
    "\n",
    "                            train_augmented_data_generator = augumentData(technique=augmentation,seed=seeds[0])\n",
    "\n",
    "                            train_generator = train_augmented_data_generator.flow_from_directory(\n",
    "                                train_dir,\n",
    "                                target_size=image_size,\n",
    "                                batch_size=batchSize,\n",
    "                                class_mode='categorical'\n",
    "                            )\n",
    "\n",
    "                            valid_generator = valid_datagen.flow_from_directory(\n",
    "                                valid_dir,\n",
    "                                target_size=image_size,\n",
    "                                batch_size=batchSize,\n",
    "                                class_mode='categorical'\n",
    "                            )\n",
    "\n",
    "                            test_generator = valid_datagen.flow_from_directory(\n",
    "                                test_dir,\n",
    "                                target_size=image_size,\n",
    "                                batch_size=batchSize,\n",
    "                                class_mode='categorical'\n",
    "                            )\n",
    "                            # create the model\n",
    "\n",
    "                            model = createModel(modelType=modelType,regularizer=regularizer,optimizer=getOptimizer(type=OptimizerType.Sgd,learningRate=learningRate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "                            # train the model \n",
    "                            model.fit(train_data_generator=train_generator,batch_size=batchSize,epochs=epochNumber,valid_data_generator=valid_generator)\n",
    "\n",
    "                            # get accuracy\n",
    "                            y_pred = model.predict(test_generator)\n",
    "                            # Convert probabilities to class labels\n",
    "                            predicted_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "                            # Get true class labels\n",
    "                            true_classes = test_generator.classes\n",
    "                            accuracy = getAccuracy(predicted_classes,true_classes)\n",
    "\n",
    "                            # append to results\n",
    "                            # It's probably easier to create a simple 2d array and then transform it to a dataframe\n",
    "                            results.append([accuracy, augmentation,batchSize,learningRate,epochNumber])\n",
    "                            # results.append({'accuracy':accuracy,'augmentation': augmentation,'batchSize':batchSize,'learningRate':learningRate,'numberOfEpochs':epochNumber})\n",
    "    return results, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class HyperParameter(Enum):\n",
    "    BatchSize=1\n",
    "    LearningRate =2\n",
    "    NumberOfEpochs =3\n",
    "\n",
    "class HyperParameters:\n",
    "    def __init__(self, batchSizes, learningRates, numberOfEpochs):\n",
    "        self.batchSizes=batchSizes\n",
    "        self.learningRates=learningRates\n",
    "        self.numberOfEpochs=numberOfEpochs\n",
    "        self.currentIndex =0\n",
    "    \n",
    "    def getNextHyperParameter(self):\n",
    "        if self.currentIndex<len(self.batchSizes):\n",
    "            return self.batchSizes[self.currentIndex], HyperParameter.BatchSize\n",
    "        elif self.currentIndex<(len(self.batchSizes+self.learningRates)):\n",
    "            return self.learningRates[self.currentIndex-len(self.batchSizes)], HyperParameter.LearningRate\n",
    "        elif self.currentIndex<(len(self.batchSizes)+len(self.learningRates)+len(self.numberOfEpochs)):\n",
    "            return self.num"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
